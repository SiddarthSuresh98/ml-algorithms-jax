# -*- coding: utf-8 -*-
"""SVM and Random Forest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ed5WtPla-ZH7HjXfWQYG9ouFV_qUInkk
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder
from tensorflow.keras import models, layers
import seaborn

df_train = pd.read_csv(r"./train.csv")
df_train.head()

df_test = pd.read_csv(r'./test.csv')
df_test.head()

df_train.info()

df_test.info()

#Split data into categorical and numerical columns
categorical_cols = [i  for i in df_train.columns if df_train[i].dtype == object]
numerical_cols = [i  for i in df_train.columns if df_train[i].dtype != object]

#Split the training dataframe into two difernt cat and num dfs
training_cat = df_train[categorical_cols]
training_num = df_train[numerical_cols]

training_num.describe()

training_num.corr()

for col in training_cat.columns:

        plt.figure(figsize=(8, 6))
        seaborn.countplot(x=col, hue=training_num['Survived'], data=training_cat)
        plt.title(f'{col} vs. Survived')
        plt.show()

for col in numerical_cols:
    plt.figure(figsize=(8, 6))
    seaborn.displot(training_num, x=col, hue='Survived', kind='kde', fill=True)
    plt.title(f'Distribution Plot of {col} by Survived')
    plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder,LabelEncoder, StandardScaler
from tensorflow.keras import models, layers

df_train = pd.read_csv(r"./train.csv")

df_train = df_train.drop(["PassengerId", "Name", "Ticket", "Cabin", "Embarked"], axis=1)
df_train = df_train.dropna(axis=0)

X_train = df_train.drop("Survived", axis=1)
y_train = df_train["Survived"]

X_test = pd.read_csv(r"./test.csv")
y_test = pd.read_csv(r"./gender_submission.csv")
df_test = pd.concat([X_test, y_test["Survived"]], axis=1)

df_test = df_test.drop(["Name", "Ticket", "Cabin", "Embarked"], axis=1)
df_test = df_test.fillna(df_test.median())
X_test_passengerId = df_test["PassengerId"]
X_test = df_test.drop(["PassengerId", "Survived"], axis=1)
y_test = df_test["Survived"]

passengerId = pd.DataFrame(X_test_passengerId)

X_train_num = X_train[["Pclass", "Age", "SibSp", "Parch", "Fare"]]
X_test_num = X_test[["Pclass","Age", "SibSp", "Parch", "Fare"]]

X_train_cat = X_train[["Sex"]]
X_test_cat = X_test[["Sex"]]

scaler = StandardScaler()
X_train_num = scaler.fit_transform(X_train_num)
X_test_num = scaler.transform(X_test_num)

encoder = LabelEncoder()
X_train_cat = encoder.fit_transform(X_train_cat)
X_test_cat = encoder.transform(X_test_cat)

X_train = np.c_[X_train_num, X_train_cat]
X_test = np.c_[X_test_num, X_test_cat]

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score

def evaluate_models(models, X, y):

    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    results = []

    for model_name, model in models.items():
        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions on the test set
        y_pred = model.predict(X_test)

        # Calculate accuracy
        accuracy = accuracy_score(y_test, y_pred)

        # Calculate precision
        precision = precision_score(y_test, y_pred)

        # Store the results
        results.append({
            'Model': model_name,
            'Accuracy': accuracy,
            'Precision': precision
        })

    return results

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV

def tune_random_forest_hyperparameters(X, y, param_grid, cv=5):
    # Create a Random Forest classifier
    rf_model = RandomForestClassifier()

    # Perform grid search
    grid_search = GridSearchCV(rf_model, param_grid, cv=cv, scoring='accuracy')
    grid_search.fit(X, y)

    # get results
    results = grid_search.cv_results_
    params = results['params']
    print(params)
    mean_test_scores = results['mean_test_score']

    # Plot results
    param_names = list(param_grid.keys())
    param_values = [param_values for param_values in zip(*[param_set.values() for param_set in params])]
    for i, param_name in enumerate(param_names):
        plt.figure(figsize=(10, 6))
        plt.plot(param_values[i], mean_test_scores, marker='o')
        plt.title(f'Mean Test Score vs {param_name}')
        plt.xlabel(param_name)
        plt.ylabel('Mean Test Score')
        plt.show()

param_grid = {
    'n_estimators': [10, 50, 100, 150, 200],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10,7],
    'min_samples_leaf': [1, 2, 4,80]
}

tune_random_forest_hyperparameters(X_train, y_train, param_grid)

rf_model = RandomForestClassifier(
    n_estimators=200,  # Number of trees in the forest
    max_depth=20,  # Maximum depth of the tree (None means nodes are expanded until they contain less than min_samples_split samples)
    min_samples_split=7,  # Minimum number of samples required to split an internal node
    min_samples_leaf=80,  # Minimum number of samples required to be at a leaf node
    max_features='auto',  # Number of features to consider when looking for the best split ('auto' means all features)
    random_state=42  # Seed for random number generation for reproducibility
)
rf_model.fit(X_train,y_train)
y_pred = rf_model.predict(X_test)
y_pred = y_pred.reshape(418)
submission = pd.DataFrame({
        "PassengerId": passengerId["PassengerId"],
        "Survived": y_pred
    })
submission.to_csv('./RFsubmission.csv', index=False)

from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, make_scorer
from sklearn.pipeline import Pipeline

random_state = 20231210

val_p = 0.4

print(np.shape(X_train))

indices = np.arange(X_train.shape[0])

ind_train, ind_val = train_test_split(indices, test_size=val_p, random_state=random_state, shuffle=True)

n_features = X_train.shape[1]

C_list = [2 ** i for i in range(-2, 3)]
gamma_list = [1 / (i * n_features) for i in np.arange(0.10, 1.75, 0.5)]
ker_list = ['rbf', 'poly', 'sigmoid', 'linear']

# A pipeline is needed to perform the scaling ofthe dataset
pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(class_weight='balanced'))])
hparameters = {'svc__kernel':ker_list, 'svc__C':C_list, 'svc__gamma':gamma_list}

svm_gs = GridSearchCV(pipe,
                      param_grid=hparameters,
                      scoring='f1_weighted',
                      return_train_score=True,
                      cv=zip([ind_train], [ind_val]),verbose=True)
svm_gs.fit(X_train, y_train)

df_results = pd.DataFrame(svm_gs.cv_results_)

display(df_results.sort_values(['rank_test_score'], ascending=True))

svm_gs.best_estimator_.fit(X_train, y_train)

y_pred_GS = svm_gs.best_estimator_.predict(X_test)

y_pred_GS = y_pred_GS.reshape(418)
submission = pd.DataFrame({
        "PassengerId": passengerId["PassengerId"],
        "Survived": y_pred_GS
    })
submission.to_csv('./SVMsubmission.csv', index=False)